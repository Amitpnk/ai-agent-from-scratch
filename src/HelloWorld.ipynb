{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGTtnsE7HsyC",
        "outputId": "15aaeb06-8281-47ac-ca79-6a343408abd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.70.0)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.51)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Downloading langgraph-0.3.29-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ormsgpack, tiktoken, langgraph-sdk, langgraph-checkpoint, langchain_openai, langgraph-prebuilt, langgraph\n",
            "Successfully installed langchain_openai-0.3.12 langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 ormsgpack-1.9.1 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai langgraph langchain_openai langchain_core graphviz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDh3xIhcHd5S"
      },
      "outputs": [],
      "source": [
        "import os,getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"open_api_key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9UuXlPjGHjea"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g2TsvgQhH6zx"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "system_message = SystemMessage(content=\"You are a helpful assistant to respond to general purpose queries.\")\n",
        "\n",
        "def assistant(state: MessagesState):\n",
        "    return {\"messages\": [llm.invoke([system_message] + state[\"messages\"])]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "a5pZy8X6H98M",
        "outputId": "66191e31-826b-44af-a7ed-a0a1dcd591e1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import START, StateGraph, END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_edge(\"assistant\", END)\n",
        "\n",
        "react_graph = builder.compile()\n",
        "\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zYtCZ4_WIdWz",
        "outputId": "e37d252f-855c-425f-8af4-0be809fd97f7"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-55f104c84782>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tell me what is the capital of India?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreact_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2718\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2719\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2357\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5d374acdf332>\u001b[0m in \u001b[0;36massistant\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMessagesState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m         return cast(\n\u001b[1;32m    330\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    893\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 results.append(\n\u001b[0;32m--> 719\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    720\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    961\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "messages = [HumanMessage(content=\"Tell me what is the capital of India?\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLCQXNiyIZEy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
